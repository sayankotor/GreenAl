{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 12:18:27.619413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#dataset_train = TextDataset(tokenizer=tokenizer, \n",
    "                                #file_path=\"/notebook/greenAI/wikitext-103/wiki.train.tokens\", \n",
    "                                #block_size=512)\n",
    "\n",
    "dataset_test = TextDataset(tokenizer=tokenizer, \n",
    "                                file_path=\"/notebook/greenAI/wikitext-103/wiki.valid.tokens\", \n",
    "                                block_size=512)\n",
    "\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 2\n",
    "args.n_gpu = 1\n",
    "args.num_train_epochs = 25\n",
    "args.seed = 42\n",
    "args.mlm = False\n",
    "args.device = device\n",
    "args.output_dir = working_dir \n",
    "args.fp16 = False\n",
    "args.max_grad_norm = 1.0\n",
    "args.logging_steps = 500.0\n",
    "args.eval_batch_size = 6\n",
    "args.save_total_limit = 2\n",
    "args.is_factorized = True\n",
    "args.local_rank = -1\n",
    "args.max_steps = -1\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from help_trainer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel\n",
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:09<00:00, 26.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(17.5525), 'loss': 2.865195578727566}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a model from the configuration\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/rank_0/checkpoint_best/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=768, dim_out=3072, rank=32, max_dim=32\n",
      "    after best_approx: dim_in=768, dim_out=3072\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (3, 2), (1, 2), (1, 3)]\n",
      "    final TTM dims:  [(8, 8), (8, 8), (12, 8), (1, 6)]\n",
      "    Original linear params: 2359296, ttm params: 166080 (x0.070)\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=3072, dim_out=768, rank=32, max_dim=32\n",
      "    after best_approx: dim_in=3072, dim_out=768\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 3), (2, 1), (3, 1)]\n",
      "    final TTM dims:  [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "    Original linear params: 2359296, ttm params: 166080 (x0.070)\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "model = GPT2_TT_Model(configuration, rank = 32, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/new_tt32/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:11<00:00, 20.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(21.0633), 'loss': 3.0475341907290163}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=768, dim_out=3072, rank=64, max_dim=64\n",
      "    after best_approx: dim_in=768, dim_out=3072\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (3, 2), (1, 2), (1, 3)]\n",
      "    final TTM dims:  [(8, 8), (8, 8), (12, 8), (1, 6)]\n",
      "    Original linear params: 2359296, ttm params: 659840 (x0.280)\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=3072, dim_out=768, rank=64, max_dim=64\n",
      "    after best_approx: dim_in=3072, dim_out=768\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 3), (2, 1), (3, 1)]\n",
      "    final TTM dims:  [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "    Original linear params: 2359296, ttm params: 659840 (x0.280)\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = GPT2_TT_Model(configuration, rank = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/new_tt64/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=768, dim_out=3072, rank=16, max_dim=16\n",
      "    after best_approx: dim_in=768, dim_out=3072\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (3, 2), (1, 2), (1, 3)]\n",
      "    final TTM dims:  [(4, 4), (4, 4), (4, 4), (4, 4), (3, 12)]\n",
      "    Original linear params: 2359296, ttm params: 13120 (x0.006)\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=3072, dim_out=768, rank=16, max_dim=16\n",
      "    after best_approx: dim_in=3072, dim_out=768\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 3), (2, 1), (3, 1)]\n",
      "    final TTM dims:  [(4, 4), (4, 4), (4, 4), (4, 4), (12, 3)]\n",
      "    Original linear params: 2359296, ttm params: 13120 (x0.006)\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "\n",
    "model = GPT2_TT_Model(configuration, rank = 16, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:10<00:00, 23.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(21.6136), 'loss': 3.0733221290541475}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(working_dir + '/new_tt16/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=768, dim_out=3072, rank=80, max_dim=80\n",
      "    after best_approx: dim_in=768, dim_out=3072\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (3, 2), (1, 2), (1, 3)]\n",
      "    final TTM dims:  [(16, 16), (16, 16), (3, 12)]\n",
      "    Original linear params: 2359296, ttm params: 1661760 (x0.704)\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=3072, dim_out=768, rank=80, max_dim=80\n",
      "    after best_approx: dim_in=3072, dim_out=768\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 3), (2, 1), (3, 1)]\n",
      "    final TTM dims:  [(16, 16), (16, 16), (12, 3)]\n",
      "    Original linear params: 2359296, ttm params: 1661760 (x0.704)\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "\n",
    "model = GPT2_TT_Model(configuration, rank = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:10<00:00, 22.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(18.3227), 'loss': 2.9081405073892874}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load('/notebook/greenAI_gitlab/greenAI_gpt/out_tt_transformer_80/checkpoint-137800/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
