{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#dataset_train = TextDataset(tokenizer=tokenizer, \n",
    "                                #file_path=\"/notebook/greenAI/wikitext-103/wiki.train.tokens\", \n",
    "                                #block_size=512)\n",
    "\n",
    "dataset_test = TextDataset(tokenizer=tokenizer, \n",
    "                                file_path=\"/notebook/greenAI/wikitext-103/wiki.valid.tokens\", \n",
    "                                block_size=1024)\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(item):\n",
    "        \"\"\"Collate function for DataLoader\n",
    "        Args:\n",
    "            item (List[dict[str, List[int]]])\n",
    "        Returns:\n",
    "            (dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        keys = item[0].keys()\n",
    "        dic = {\n",
    "            key: torch.tensor([x[key] for x in item])\n",
    "            for key in keys\n",
    "        }\n",
    "        return dic\n",
    "    \n",
    "def __len__(self):\n",
    "        return len(self.filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 220,  198,  796,  ...,  262, 2619, 6896],\n",
       "        [ 764,  383, 7840,  ...,  290,  262, 1279],\n",
       "        [2954,   29,  423,  ...,  764,  679,  373],\n",
       "        ...,\n",
       "        [ 358, 1812, 3845,  ...,  837,  351, 6135],\n",
       "        [2488,   13,   31,  ..., 2700,  373,  691],\n",
       "        [1315, 2488,   11,  ..., 8069,  290,  468]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(dataset=dataset_test[:15], \n",
    "                                             batch_size=5, \n",
    "                                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 2\n",
    "args.n_gpu = 1\n",
    "args.num_train_epochs = 25\n",
    "args.seed = 42\n",
    "args.mlm = False\n",
    "args.device = device\n",
    "args.output_dir = working_dir \n",
    "args.fp16 = False\n",
    "args.max_grad_norm = 1.0\n",
    "args.logging_steps = 500.0\n",
    "args.eval_batch_size = 6\n",
    "args.save_total_limit = 2\n",
    "args.is_factorized = True\n",
    "args.local_rank = -1\n",
    "args.max_steps = -1\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: path in /opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages (16.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/.pyenv/versions/3.8.10/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current = globals()['_dh'][0]\n",
    " \n",
    "# Getting the parent directory name\n",
    "# where the current directory is present.\n",
    "parent = os.path.dirname(current)\n",
    " \n",
    "# adding the parent directory to\n",
    "# the sys.path.\n",
    "sys.path.append(parent)\n",
    "from help_trainer import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel\n",
    "from src.classes.gpt_med_config import GPT2MedConfig\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2MedConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 243/243 [00:14<00:00, 16.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(21.3699), 'loss': 3.061984294726525}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a model from the configuration\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages (4.64.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/.pyenv/versions/3.8.10/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    eval_loss = eval_loss / nb_eval_steps\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, sample in tqdm(enumerate(val_dataloader, start=1)):\n",
    "        input_ids, label_ids = sample[:seq_len], sample[1:seq_len+1]\n",
    "        input_ids = input_ids.to(device=device)\n",
    "        label_ids = label_ids.to(device=device)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    #perplexity = perplexity / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18481/1286352828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "eval_loss = 0.0\n",
    "perplexity = 0.0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, sample in tqdm(enumerate(val_dataloader, start=1)):\n",
    "        input_ids, label_ids = sample[:seq_len], sample[1:seq_len+1]\n",
    "        input_ids = input_ids.to(device=device)\n",
    "        label_ids = label_ids.to(device=device)\n",
    "        # average(on number of tokens) negative log-likelihood for each token is returned as the loss\n",
    "        outputs = model(input_ids)\n",
    "        print (len(outputs[0]), len(outputs[1]))\n",
    "        lm_loss = outputs[0]\n",
    "        eval_loss += lm_loss.mean().item()\n",
    "        perplexity += torch.exp(torch.tensor(eval_loss))\n",
    "        nb_eval_steps += 1\n",
    "    model.train()\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    #perplexity = perplexity / nb_eval_steps\n",
    "    print (\"loss\", eval_loss)\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "print (perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=1024, dim_out=4096, rank=72, max_dim=72\n",
      "    after best_approx: dim_in=1024, dim_out=4096\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (1, 2), (1, 2)]\n",
      "    final TTM dims:  [(16, 16), (16, 16), (4, 16)]\n",
      "    Original linear params: 4194304, ttm params: 1350144 (x0.322)\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=4096, dim_out=1024, rank=72, max_dim=72\n",
      "    after best_approx: dim_in=4096, dim_out=1024\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 1), (2, 1)]\n",
      "    final TTM dims:  [(16, 16), (16, 16), (16, 4)]\n",
      "    Original linear params: 4194304, ttm params: 1350144 (x0.322)\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2_TT_Model(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x4x16x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (c_proj): FactorizationTTMLinear(\n",
       "            (ttm): TTM(\n",
       "              (tt): PlainTTMContainer(\n",
       "                (cores): ParameterList(\n",
       "                    (0): Parameter containing: [torch.cuda.FloatTensor of size 1x16x16x72 (GPU 2)]\n",
       "                    (1): Parameter containing: [torch.cuda.FloatTensor of size 72x16x16x72 (GPU 2)]\n",
       "                    (2): Parameter containing: [torch.cuda.FloatTensor of size 72x16x4x1 (GPU 2)]\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.classes.gpt2_tt import GPT2_TT_Model\n",
    "configuration = GPT2MedConfig()\n",
    "\n",
    "import torch.distributed as dist\n",
    "   \n",
    "model = GPT2_TT_Model(configuration, rank = 72)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218303488\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 243/243 [00:17<00:00, 13.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(38.4058), 'loss': 3.6482085612575705}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load('/notebook/greenAI/out_transformer_0_v4/checkpoint-6000/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids[0][-10:]  section east of Crete, and along only the north @-@ west coast of the Black Sea\n",
      "\n",
      "\n",
      "label_ids[0][-11:]  the section east of Crete, and along only the north @-@ west coast of the Black Sea\n",
      "\n",
      "\n",
      "output  of of thete. and is the the eastern coast-@ eastern coast of the Black Sea.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids[0][-10:]  Hesperia to New Era, bypassing Ferry. A larger change around 1969 reconfigured the highway design\n",
      "\n",
      "\n",
      "label_ids[0][-11:]  from Hesperia to New Era, bypassing Ferry. A larger change around 1969 reconfigured the highway design\n",
      "\n",
      "\n",
      "output peria to Howarday. replacinging the,  new section was the wasfigured the highway to to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids[0][-10:] \n",
      " = = = Historic districts = = = \n",
      " \n",
      " Meridian has nine historic districts that are\n",
      "\n",
      "\n",
      "label_ids[0][-11:]  \n",
      " = = = Historic districts = = = \n",
      " \n",
      " Meridian has nine historic districts that are\n",
      "\n",
      "\n",
      "output  The = = Hot buildings = =  FactoryReloaded @#& The Main been historic districts, are part\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.to(device).eval()\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, sample in tqdm(enumerate(val_dataloader, start=1)):\n",
    "        input_ids, label_ids = sample[:seq_len], sample[1:seq_len+1]\n",
    "        input_ids = input_ids.to(device=device)\n",
    "        label_ids = label_ids.to(device=device)\n",
    "        # average(on number of tokens) negative log-likelihood for each token is returned as the loss\n",
    "        outputs = model(input_ids)\n",
    "        #print(\"INPUT\", tokenizer.decode(input_ids[0]), \"\\n\")\n",
    "        #print(\"REAL\", tokenizer.decode(label_ids[0]), \"\\n\")\n",
    "        print (\"\\n\")\n",
    "        print(\"input_ids[0][-10:]\", tokenizer.decode(input_ids[0][-20:]))\n",
    "        print (\"\\n\")\n",
    "        print(\"label_ids[0][-11:]\", tokenizer.decode(input_ids[0][-21:]))\n",
    "        print (\"\\n\")\n",
    "        print(\"output\", tokenizer.decode(outputs.logits.argmax(dim=-1)[0][-20:]))\n",
    "        #print(\"GENERATED\", tokenizer.decode(outputs.logits.argmax(dim=-1)[0][-1:]))\n",
    "        print(\"\\n\\n\\n\")\n",
    "        i += 1\n",
    "        if i > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bookcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test_ds = load_dataset(\"bookcorpus\",split='train[:4782]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4782"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['usually , he would be tearing around the living room , playing with his toys .',\n",
       "  'but just one look at a minion sent him practically catatonic .',\n",
       "  \"that had been megan 's plan when she got him dressed earlier .\",\n",
       "  \"he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\",\n",
       "  'she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .',\n",
       "  \"`` are n't you being a good boy ? ''\",\n",
       "  'she said .',\n",
       "  'mason barely acknowledged her .',\n",
       "  'instead , his baby blues remained focused on the television .',\n",
       "  'since the movie was almost over , megan knew she better slip into the bedroom and finish getting ready .']}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2b613eb1de4a5b9c4cecef8cb99433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4782"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = test_ds.map(lambda examples: tokenizer(examples['text'], max_length=1025, truncation=True), batched=True)\n",
    "len(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'usually , he would be tearing around the living room , playing with his toys .',\n",
       " 'input_ids': [23073,\n",
       "  837,\n",
       "  339,\n",
       "  561,\n",
       "  307,\n",
       "  24447,\n",
       "  1088,\n",
       "  262,\n",
       "  2877,\n",
       "  2119,\n",
       "  837,\n",
       "  2712,\n",
       "  351,\n",
       "  465,\n",
       "  14958,\n",
       "  764],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BookCorpusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.encodings)\n",
    "        item = {}\n",
    "        item['decoder_attention_mask'] = self.encodings[idx]['attention_mask']\n",
    "        item['input_ids'] =  self.encodings[idx]['input_ids']\n",
    "        return item       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "val_dataset = BookCorpusDataset(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(item):\n",
    "    \"\"\"Collate function for DataLoader\n",
    "    Args:\n",
    "        item (List[dict[str, List[int]]])\n",
    "    Returns:\n",
    "        (dict[str, torch.Tensor]):\n",
    "    \"\"\"\n",
    "    keys = item[0].keys()\n",
    "    dic = {\n",
    "        key: torch.tensor([x[key] for x in item])\n",
    "        for key in keys\n",
    "    }\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn = collate_fn,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4782"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "input_ids[0][-20:]  around the living room, playing with his toys.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  tearing around the living room, playing with his toys.\n",
      "\n",
      "\n",
      "output  in edges room, and with his hands,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  at a minion sent him practically catatonic.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  look at a minion sent him practically catatonic.\n",
      "\n",
      "\n",
      "output  the few. to to intoatonic..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] 's plan when she got him dressed earlier.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] gan's plan when she got him dressed earlier.\n",
      "\n",
      "\n",
      "output em. for I was the. up this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "input_ids[0][-20:] ason was often exposed to things that were older.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  mason was often exposed to things that were older.\n",
      "\n",
      "\n",
      "output  and a seen to the that were not than He\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  a such a good talker for his age.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  was a such a good talker for his age.\n",
      "\n",
      "\n",
      "output  good a good kider. the age..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  aren't you being a good boy? ''\n",
      "\n",
      "\n",
      "label_ids[0][-21:] `` aren't you being a good boy? ''\n",
      "\n",
      "\n",
      "output  theameless the? a little person?\"\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] she said.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] she said.\n",
      "\n",
      "\n",
      "output  was..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "input_ids[0][-20:] mason barely acknowledged her.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] mason barely acknowledged her.\n",
      "\n",
      "\n",
      "output ., survived the own.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] , his baby blues remained focused on the television.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] instead, his baby blues remained focused on the television.\n",
      "\n",
      "\n",
      "output  the wife was, the on the music show He\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  better slip into the bedroom and finish getting ready.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  she better slip into the bedroom and finish getting ready.\n",
      "\n",
      "\n",
      "output  not into the role and get the ready for I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  was grateful that he looked nothing like his father.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  she was grateful that he looked nothing like his father.\n",
      "\n",
      "\n",
      "output  still for she was at like her...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch_idx, sample in tqdm(enumerate(val_dataloader)):\n",
    "    #print(batch_idx, sample)\n",
    "    input_ids, label_ids = sample['input_ids'][0][:seq_len - 1], sample['input_ids'][0][1:seq_len]\n",
    "    #input_ids, label_ids = sample[:seq_len], sample[1:seq_len+1]\n",
    "    input_ids = input_ids.to(device=device)\n",
    "    label_ids = label_ids.to(device=device)\n",
    "    # average(on number of tokens) negative log-likelihood for each token is returned as the loss\n",
    "    outputs = model(input_ids)\n",
    "    #print(\"INPUT\", tokenizer.decode(input_ids[0]), \"\\n\")\n",
    "    #print(\"REAL\", tokenizer.decode(label_ids[0]), \"\\n\")\n",
    "    print (\"\\n\")\n",
    "    print(\"input_ids[0][-20:]\", tokenizer.decode(input_ids[-10:]))\n",
    "    print (\"\\n\")\n",
    "    print(\"label_ids[0][-21:]\", tokenizer.decode(input_ids[-11:]))\n",
    "    print (\"\\n\")\n",
    "    print(\"output\", tokenizer.decode(outputs.logits.argmax(dim=-1)[-10:]))\n",
    "    #print(\"GENERATED\", tokenizer.decode(outputs.logits.argmax(dim=-1)[0][-1:]))\n",
    "    print(\"\\n\\n\\n\")\n",
    "    i += 1\n",
    "    if i > 10: break\n",
    "    print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 25.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "input_ids[0][-20:]  around the living room, playing with his toys.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  tearing around the living room, playing with his toys.\n",
      "\n",
      "\n",
      "output  in edges room, and with his hands,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  at a minion sent him practically catatonic.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  look at a minion sent him practically catatonic.\n",
      "\n",
      "\n",
      "output  the few. to to intoatonic..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] 's plan when she got him dressed earlier.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] gan's plan when she got him dressed earlier.\n",
      "\n",
      "\n",
      "output em. for I was the. up this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] ason was often exposed to things that were older.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  mason was often exposed to things that were older.\n",
      "\n",
      "\n",
      "output  and a seen to the that were not than He\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  a such a good talker for his age.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  was a such a good talker for his age.\n",
      "\n",
      "\n",
      "output  good a good kider. the age..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  aren't you being a good boy? ''\n",
      "\n",
      "\n",
      "label_ids[0][-21:] `` aren't you being a good boy? ''\n",
      "\n",
      "\n",
      "output "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 22.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " theameless the? a little person?\"\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] she said.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] she said.\n",
      "\n",
      "\n",
      "output  was..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] mason barely acknowledged her.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] mason barely acknowledged her.\n",
      "\n",
      "\n",
      "output ., survived the own.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] , his baby blues remained focused on the television.\n",
      "\n",
      "\n",
      "label_ids[0][-21:] instead, his baby blues remained focused on the television.\n",
      "\n",
      "\n",
      "output  the wife was, the on the music show He\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  better slip into the bedroom and finish getting ready.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  she better slip into the bedroom and finish getting ready.\n",
      "\n",
      "\n",
      "output  not into the role and get the ready for I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  was grateful that he looked nothing like his father.\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  she was grateful that he looked nothing like his father.\n",
      "\n",
      "\n",
      "output  still for she was at like her...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch_idx, sample in tqdm(enumerate(val_dataloader)):\n",
    "    #print(batch_idx, sample)\n",
    "    input_ids, label_ids = sample['input_ids'][0][:seq_len - 1], sample['input_ids'][0][1:seq_len]\n",
    "    #input_ids, label_ids = sample[:seq_len], sample[1:seq_len+1]\n",
    "    input_ids = input_ids.to(device=device)\n",
    "    label_ids = label_ids.to(device=device)\n",
    "    # average(on number of tokens) negative log-likelihood for each token is returned as the loss\n",
    "    outputs = model(input_ids)\n",
    "    #print(\"INPUT\", tokenizer.decode(input_ids[0]), \"\\n\")\n",
    "    #print(\"REAL\", tokenizer.decode(label_ids[0]), \"\\n\")\n",
    "    print (\"\\n\")\n",
    "    print(\"input_ids[0][-20:]\", tokenizer.decode(input_ids[-10:]))\n",
    "    print (\"\\n\")\n",
    "    print(\"label_ids[0][-21:]\", tokenizer.decode(input_ids[-11:]))\n",
    "    print (\"\\n\")\n",
    "    print(\"output\", tokenizer.decode(outputs.logits.argmax(dim=-1)[-10:]))\n",
    "    #print(\"GENERATED\", tokenizer.decode(outputs.logits.argmax(dim=-1)[0][-1:]))\n",
    "    print(\"\\n\\n\\n\")\n",
    "    i += 1\n",
    "    if i > 10: break\n",
    "    print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.eval_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740042"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'slice' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18481/4289245355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18481/3958314074.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'slice' and 'int'"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 5it [00:00, 22.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "input_ids[0][-20:]  tearing around the living room, playing with his toys\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  be tearing around the living room, playing with his toys\n",
      "\n",
      "\n",
      "output  up in edges room, and with his hands,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  look at a minion sent him practically catatonic\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  one look at a minion sent him practically catatonic\n",
      "\n",
      "\n",
      "output  at the few. to to intoatonic.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] gan's plan when she got him dressed earlier\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  megan's plan when she got him dressed earlier\n",
      "\n",
      "\n",
      "output icallyem. for I was the. up this\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  mason was often exposed to things that were older\n",
      "\n",
      "\n",
      "label_ids[0][-21:] , mason was often exposed to things that were older\n",
      "\n",
      "\n",
      "output ister and a seen to the that were not than\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  was a such a good talker for his age\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  he was a such a good talker for his age\n",
      "\n",
      "\n",
      "output  so good a good kider. the age.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 8it [00:00, 22.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "input_ids[0][-20:] `` aren't you being a good boy?\n",
      "\n",
      "\n",
      "label_ids[0][-21:] `` aren't you being a good boy?\n",
      "\n",
      "\n",
      "output The theameless the? a little person?\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] she said\n",
      "\n",
      "\n",
      "label_ids[0][-21:] she said\n",
      "\n",
      "\n",
      "output  was.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] mason barely acknowledged her\n",
      "\n",
      "\n",
      "label_ids[0][-21:] mason barely acknowledged her\n",
      "\n",
      "\n",
      "output ., survived the own\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:] instead, his baby blues remained focused on the television\n",
      "\n",
      "\n",
      "label_ids[0][-21:] instead, his baby blues remained focused on the television\n",
      "\n",
      "\n",
      "output  of the wife was, the on the music show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input_ids[0][-20:]  she better slip into the bedroom and finish getting ready\n",
      "\n",
      "\n",
      "label_ids[0][-21:]  knew she better slip into the bedroom and finish getting ready\n",
      "\n",
      "\n",
      "output  was not into the role and get the ready for\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 2569it [01:32, 16.98it/s]"
     ]
    }
   ],
   "source": [
    "eval_loss = 0.0\n",
    "perplexity = 0.0\n",
    "nb_eval_steps = 1\n",
    "model.eval()\n",
    "i = 0\n",
    "losses = []\n",
    "for idx, sample in tqdm(enumerate(val_dataloader), desc=\"Evaluating\"):\n",
    "    input_ids = sample['input_ids'][0][:seq_len]\n",
    "    label_ids = sample['input_ids'][0][1:len(input_ids)]\n",
    "    input_ids = input_ids[:len(input_ids) - 1]\n",
    "    #input_ids, label_ids = sample[:seq_len], sample[1:seq_len+1]\n",
    "    input_ids = input_ids.to(device=device)\n",
    "    label_ids = label_ids.to(device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=label_ids)\n",
    "        if i < 10:\n",
    "            print (\"\\n\")\n",
    "            print(\"input_ids[0][-20:]\", tokenizer.decode(input_ids[-10:]))\n",
    "            print (\"\\n\")\n",
    "            print(\"label_ids[0][-21:]\", tokenizer.decode(input_ids[-11:]))\n",
    "            print (\"\\n\")\n",
    "            print(\"output\", tokenizer.decode(outputs.logits.argmax(dim=-1)[-10:]))\n",
    "            #print(\"GENERATED\", tokenizer.decode(outputs.logits.argmax(dim=-1)[0][-1:]))\n",
    "            print(\"\\n\\n\\n\")\n",
    "            i += 1\n",
    "        \n",
    "        lm_loss = outputs[0]\n",
    "        eval_loss += lm_loss.mean().item()\n",
    "        perplexity += torch.exp(torch.tensor(eval_loss))\n",
    "        losses.append(eval_loss)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "#perplexity = perplexity / nb_eval_steps\n",
    "perplexity = torch.exp(torch.tensor(eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
