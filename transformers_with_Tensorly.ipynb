{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Tuple\n",
    "\n",
    "def factorize(n: int) -> List[int]:\n",
    "    if n == 1:\n",
    "        return [1]\n",
    "    factors = []\n",
    "    i = 2\n",
    "    while i * i <= n:\n",
    "        while n % i == 0:\n",
    "            factors.append(i)\n",
    "            n //= i\n",
    "        i += 1\n",
    "    if n != 1:\n",
    "        factors.append(n)\n",
    "    return factors\n",
    "\n",
    "def log2(n: int, assert_eq: bool):\n",
    "    res = 0\n",
    "    while 2 ** res < n:\n",
    "        res += 1\n",
    "\n",
    "    if assert_eq:\n",
    "        assert 2 ** res == n\n",
    "\n",
    "    return res\n",
    "\n",
    "def best_approx(n: int, max_factor: int = 3):\n",
    "    n_factors = log2(n, False)\n",
    "    print (\"n_factors\", n_factors)\n",
    "    while True:\n",
    "        factors = factorize(n)\n",
    "        print (\"len(factors)\", len(factors))\n",
    "        if len(factors) <= n_factors and all([f <= max_factor for f in factors]):\n",
    "            return n\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tltorch.factorized_layers import FactorizedLinear\n",
    "from tltorch.factorized_tensors import FactorizedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tltorch.tensor_hooks import tensor_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FactorizedTensor (tensor_dropout can be applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inpt = torch.zeros(size = (3072, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactorList(\n",
      "    (factor_0): Parameter containing: [torch.FloatTensor of size 1x16x64]\n",
      "    (factor_1): Parameter containing: [torch.FloatTensor of size 64x16x64]\n",
      "    (factor_2): Parameter containing: [torch.FloatTensor of size 64x256x64]\n",
      "    (factor_3): Parameter containing: [torch.FloatTensor of size 64x36x1]\n",
      ")\n",
      "tensor dropout\n",
      "apply TTTensor\n",
      "apply TTTensor\n",
      "apply <tltorch.tensor_hooks._tensor_dropout.TTDropout object at 0x7f2a8403c350>\n",
      "apply <torch.utils.hooks.RemovableHandle object at 0x7f2b8f4bddd0>\n",
      "FactorList(\n",
      "    (factor_0): Parameter containing: [torch.FloatTensor of size 1x16x64]\n",
      "    (factor_1): Parameter containing: [torch.FloatTensor of size 64x16x64]\n",
      "    (factor_2): Parameter containing: [torch.FloatTensor of size 64x256x64]\n",
      "    (factor_3): Parameter containing: [torch.FloatTensor of size 64x36x1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tensor = FactorizedTensor.new(shape = [16, 16, 256, 36], rank=[1, 64, 64, 64, 1], factorization='TT').normal_()\n",
    "print (tensor.factors)\n",
    "tensor = tensor_dropout(tensor, p=0.5)\n",
    "print (tensor.factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index torch.Size([16])\n",
      "factor.shape torch.Size([1, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TTTensor(shape=(16, 16, 256, 36), rank=(1, 64, 64, 64, 1))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inpt = torch.zeros(size = (1, 16))\n",
    "\n",
    "tensor(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index torch.Size([3072])\n",
      "factor.shape torch.Size([1, 16, 64])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-655916e5fa9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3072\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/torch/tltorch/factorized_tensors/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, indices, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/torch/tltorch/factorized_tensors/factorized_tensors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    395\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                             \u001b[0mfactors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                             \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/torch/tltorch/utils/parameter_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "inpt = torch.zeros(size = (768, 3072))\n",
    "tensor(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorList(\n",
       "    (factor_0): Parameter containing: [torch.FloatTensor of size 1x16x64]\n",
       "    (factor_1): Parameter containing: [torch.FloatTensor of size 64x16x64]\n",
       "    (factor_2): Parameter containing: [torch.FloatTensor of size 64x256x64]\n",
       "    (factor_3): Parameter containing: [torch.FloatTensor of size 64x36x1]\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor dropout\n",
      "apply CPTensor\n",
      "apply CPTensor\n",
      "apply <tltorch.tensor_hooks._tensor_dropout.CPDropout object at 0x7f2b98b38250>\n",
      "apply <torch.utils.hooks.RemovableHandle object at 0x7f2a8403c9d0>\n"
     ]
    }
   ],
   "source": [
    "tensor = FactorizedTensor.new((30, 40, 20), rank=0.5, factorization='CP').normal_()\n",
    "inpt = torch.zeros(size = (1,133))\n",
    "tensor(inpt)\n",
    "tensor = tensor_dropout(tensor, p=0.5)\n",
    "#remove_tensor_dropout(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorList(\n",
       "    (factor_0): Parameter containing: [torch.FloatTensor of size 30x133]\n",
       "    (factor_1): Parameter containing: [torch.FloatTensor of size 40x133]\n",
       "    (factor_2): Parameter containing: [torch.FloatTensor of size 20x133]\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTTensor(shape=(16, 16, 256, 36), rank=(1, 64, 64, 64, 1))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FactorizedLinear (can be build-in into existing model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tltorch.factorized_layers.factorized_linear import FactorizedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3072, 768]\n"
     ]
    }
   ],
   "source": [
    "old_layer = model.transformer.h[2].mlp.c_fc\n",
    "(in_, out_) = old_layer.weight.shape\n",
    "itf = (16,16,3)\n",
    "otf = (16,16,12)\n",
    "layer = FactorizedLinear(in_tensorized_features=itf, out_tensorized_features=otf, rank=[1, 64, 64, 1], factorization = 'blocktt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inpt = torch.zeros(size = (3072, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0189,  0.0063,  0.0097,  ..., -0.0006, -0.0299, -0.0122],\n",
       "        [ 0.0189,  0.0063,  0.0097,  ..., -0.0006, -0.0299, -0.0122],\n",
       "        [ 0.0189,  0.0063,  0.0097,  ..., -0.0006, -0.0299, -0.0122],\n",
       "        ...,\n",
       "        [ 0.0189,  0.0063,  0.0097,  ..., -0.0006, -0.0299, -0.0122],\n",
       "        [ 0.0189,  0.0063,  0.0097,  ..., -0.0006, -0.0299, -0.0122],\n",
       "        [ 0.0189,  0.0063,  0.0097,  ..., -0.0006, -0.0299, -0.0122]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizedLinear(in_features=768, out_features=3072, weight of size (3072, 768) tensorized to ((16, 16, 12), (16, 16, 3)),factorization=BlockTT, rank=[1, 64, 64, 1], with a single layer parametrized, "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
