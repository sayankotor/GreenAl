{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tltorch.factorized_tensors.core import FactorizedTensor\n",
    "from tltorch.tensor_hooks._tensor_dropout import tensor_dropout, remove_tensor_dropout, CPDropout, TTDropout\n",
    "\n",
    "shape = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3]\n",
    "rank = [1, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,128, 128, 128, 1]\n",
    "tensor = FactorizedTensor.new(shape, rank, factorization='TT')\n",
    "\n",
    "tensor.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTTensor(shape=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3), rank=(1, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorList(\n",
       "    (factor_0): Parameter containing: [torch.FloatTensor of size 1x2x128]\n",
       "    (factor_1): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_2): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_3): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_4): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_5): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_6): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_7): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_8): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_9): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_10): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_11): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_12): Parameter containing: [torch.FloatTensor of size 128x3x128]\n",
       "    (factor_13): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_14): Parameter containing: [torch.FloatTensor of size 128x2x128]\n",
       "    (factor_15): Parameter containing: [torch.FloatTensor of size 128x3x1]\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTD dropout True\n"
     ]
    }
   ],
   "source": [
    "dout = TTDropout(0.4, min_dim=2)\n",
    "tensor1 = dout._apply_tensor_dropout(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 128])\n",
      "torch.Size([72, 2, 74])\n"
     ]
    }
   ],
   "source": [
    "print (tensor.factors[1].shape)\n",
    "print (tensor1.factors[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTTensor(shape=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3), rank=(1, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTTensor(shape=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3), rank=(1, 72, 74, 81, 72, 79, 81, 77, 83, 67, 85, 82, 80, 74, 79, 76, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTDropout(nn.Module):\n",
    "    def __init__(self, training: bool, rank: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, tt_tensor):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, proba, min_dim=1, min_values=1, drop_test=False):\n",
    "        assert 0 <= proba < 1, f'Got prob={proba} but tensor dropout is defined for 0 <= proba < 1.'\n",
    "        self.proba = proba\n",
    "        self.min_dim = min_dim\n",
    "        self.min_values = min_values\n",
    "        self.drop_test = drop_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorDropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0640f3fc7552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTTDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTTTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_tensor_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"TTD dropout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"self.min_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorDropout' is not defined"
     ]
    }
   ],
   "source": [
    "class TTDropout(TensorDropout, factorization=TTTensor):\n",
    "    def __init__(self, proba, min_dim=1, min_values=1, drop_test=False):\n",
    "        assert 0 <= proba < 1, f'Got prob={proba} but tensor dropout is defined for 0 <= proba < 1.'\n",
    "        self.proba = proba\n",
    "        self.min_dim = min_dim\n",
    "        self.min_values = min_values\n",
    "        self.drop_test = drop_test\n",
    "    \n",
    "    \n",
    "    def _apply_tensor_dropout(self, tt_tensor, training=True):\n",
    "        print (\"TTD dropout\", training)\n",
    "        print (\"rank\", rank, \"self.min_dim\", self.min_dim, rank > self.min_dim)\n",
    "        if (not self.proba) or ((not training) and (not self.drop_test)):\n",
    "            return tt_tensor\n",
    "\n",
    "        device = tt_tensor.factors[0].device\n",
    "\n",
    "        sampled_indices = []\n",
    "        for i, rank in enumerate(tt_tensor.rank[1:]):\n",
    "            if rank > self.min_dim:\n",
    "                idx = tl.arange(rank, device=device, dtype=torch.int64)\n",
    "                idx = idx[torch.bernoulli(torch.ones(rank, device=device)*(1 - self.proba),\n",
    "                                      out=torch.empty(rank, device=device, dtype=torch.bool))]\n",
    "                if len(idx) == 0:\n",
    "                    idx = torch.randint(0, rank, size=(self.min_values, ), device=device, dtype=torch.int64)\n",
    "            else:\n",
    "                idx = tl.arange(rank, device=device, dtype=torch.int64).tolist()\n",
    "\n",
    "            sampled_indices.append(idx)\n",
    "\n",
    "        sampled_factors = []\n",
    "        if training:\n",
    "            scaling = 1/(1 - self.proba)\n",
    "        else:\n",
    "            scaling = 1\n",
    "        for i, f in enumerate(tt_tensor.factors):\n",
    "            if i == 0:\n",
    "                sampled_factors.append(f[..., sampled_indices[i]]*scaling)\n",
    "            elif i == (tt_tensor.order - 1):\n",
    "                sampled_factors.append(f[sampled_indices[i-1], ...])\n",
    "            else:\n",
    "                sampled_factors.append(f[sampled_indices[i-1], ...][..., sampled_indices[i]]*scaling)\n",
    "\n",
    "        return TTTensor(sampled_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTMDropout(nn.Module):\n",
    "    def __init__(self, prob: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_in_ttm = best_approx(d_in)\n",
    "        self.d_out = d_out\n",
    "        self.d_out_ttm = best_approx(d_out)\n",
    "\n",
    "        self.ttm = TTM(self.d_in_ttm, self.d_out_ttm, rank)\n",
    "\n",
    "    def forward(self, x: t.tensor, prob: int):\n",
    "        dout = TTDropout(0.4, min_dim=2)\n",
    "        tensor1 = dout._apply_tensor_dropout(tensor)\n",
    "        return tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTContainer(nn.Module):\n",
    "    def __init__(self, dims: List[int], rank_or_ranks: Union[int, List[int]]):\n",
    "        super().__init__()\n",
    "\n",
    "        ranks = rank_or_ranks if isinstance(rank_or_ranks, list) else [rank_or_ranks] * (len(dims) - 1)\n",
    "\n",
    "        assert len(dims) == len(ranks) + 1\n",
    "\n",
    "        self.cores = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(t.randn(r1, dim, r2) / np.sqrt(dim * r1), requires_grad=True)\n",
    "                for dim, r1, r2 in zip(dims, [1] + ranks, ranks + [1])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def n_dims(self):\n",
    "        return len(self.cores)\n",
    "\n",
    "\n",
    "class TTM(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, rank: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.dims_in = tuple(factorize(dim_in))\n",
    "        self.dims_out = tuple(factorize(dim_out))\n",
    "        self.dims = mix(self.dims_in, self.dims_out)\n",
    "\n",
    "        self.tt = TTContainer(self.dims, rank)\n",
    "\n",
    "    def get_in_out_dim_inds(self) -> Tuple[List[int], List[int]]:\n",
    "        return unmix(list(range(len(self.dims_in) + len(self.dims_out))), len(self.dims_in), len(self.dims_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ein_str_parts = []\n",
    "\n",
    "        for i, cores in enumerate(self.tt.cores):\n",
    "            r1 = opt_einsum.get_symbol(self.tt.n_dims + i)\n",
    "            dim = opt_einsum.get_symbol(i)\n",
    "            r2 = opt_einsum.get_symbol(self.tt.n_dims + i + 1)\n",
    "\n",
    "            ein_str_parts.append(f'{r1}{dim}{r2}')\n",
    "\n",
    "        in_inds, out_inds = self.get_in_out_dim_inds()\n",
    "\n",
    "        batch_dim = self.tt.n_dims * 2 + 2\n",
    "        x_dims = [batch_dim] + in_inds\n",
    "        ein_str_parts.append(''.join(opt_einsum.get_symbol(dim) for dim in x_dims))\n",
    "\n",
    "        ein_str_parts.append(''.join(opt_einsum.get_symbol(dim) for dim in out_inds))\n",
    "        ein_str = f'{\",\".join(ein_str_parts[:-1])}->{opt_einsum.get_symbol(batch_dim)}{ein_str_parts[-1]}'\n",
    "        x_reshaped = x.reshape(x.shape[:1] + self.dims_in) # [4096, 2, 2, 2, 2, 2, 2, 2, 2, 3]\n",
    "        a = cached_einsum(ein_str, *self.tt.cores, x_reshaped) # [4096, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3]\n",
    "        a = a.reshape(x_reshaped.shape[0], -1) #([4096, 3072]) might be ([4096, 768])\n",
    "        \n",
    "        return cached_einsum(ein_str, *self.tt.cores, x_reshaped).reshape(x_reshaped.shape[0], -1)\n",
    "\n",
    "    def full_tensor(self) -> t.Tensor:\n",
    "        ds = [opt_einsum.get_symbol(i) for i in range(len(self.dims))]\n",
    "        rs = [opt_einsum.get_symbol(len(self.dims) + i) for i in range(len(self.dims) + 1)]\n",
    "\n",
    "        left = ','.join(f'{r1}{d}{r2}' for r1, d, r2 in zip(rs, ds, rs[1:]))\n",
    "        right = ''.join(sum(unmix(ds, len(self.dims_in), len(self.dims_out)), []))\n",
    "\n",
    "        return cached_einsum(f'{left}->{right}', *self.tt.cores).reshape(self.dim_in, self.dim_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
